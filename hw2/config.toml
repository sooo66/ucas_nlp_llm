[paths]
dataset = "news.2017.zh.shuffled.deduped"
tokenizer_name = "bert-base-chinese"
output_dir = "runs"

[training]
epochs = 5
batch_size = 32
num_workers = 2
learning_rate = 0.0003
weight_decay = 0.01
grad_clip = 1.0
seq_len = 128
log_interval = 100
seed = 42
device = "cuda"

[fnn]
context_size = 5
embedding_dim = 256
hidden_dim = 512

[rnn]
embedding_dim = 256
hidden_dim = 512

[transformer]
d_model = 256
num_heads = 4
num_layers = 4
ffn_dim = 1024
dropout = 0.1

[logging]
log_level = "INFO"

[experiments]
fnn_context_sizes = [3, 5, 7]
max_lengths = [64, 128, 256]
cuda_devices = ["0", "1", "2"]
